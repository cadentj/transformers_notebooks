{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "\n",
    "from typing import Float, List, Tuple\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n",
    "\n",
    "Here we make some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "def generate_string_manipulation_data(num_examples):\n",
    "    examples = []\n",
    "    for _ in range(num_examples):\n",
    "        input_string = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 10)))\n",
    "        # instructions = random.choice(['reverse', 'upper', 'lower', 'capital', 'swap', 'echo'])\n",
    "        instructions = random.choice(['upper', 'echo'])\n",
    "\n",
    "        if instructions == 'reverse':\n",
    "            output_string = input_string[::-1]\n",
    "        elif instructions == 'upper':\n",
    "            output_string = input_string.upper()\n",
    "        elif instructions == 'lower':\n",
    "            output_string = input_string.lower()\n",
    "        elif instructions == 'capital':\n",
    "            output_string = input_string.capitalize()\n",
    "        elif instructions == 'swap':\n",
    "            output_string = input_string.swapcase()\n",
    "        elif instructions == 'echo':\n",
    "            output_string = input_string\n",
    "\n",
    "        examples.append(f\"{input_string} {instructions} {output_string}. \")\n",
    "\n",
    "    return examples\n",
    "\n",
    "print('\\n'.join(generate_string_manipulation_data(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally our special words \"upper\" and \"echo\" would just be a single token.\n",
    "We use a \"Byte Pair Encoding\" tokenizer to learn these rules based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Split\n",
    "\n",
    "# Create a new CharBPETokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Split(' ', 'isolated')\n",
    "\n",
    "# Train the tokenizer on the synthetic dataset\n",
    "trainer = BpeTrainer(vocab_size=100, min_frequency=2, special_tokens=[\"<unk>\"])\n",
    "tokenizer.train_from_iterator(generate_string_manipulation_data(1000), trainer=trainer)\n",
    "\n",
    "# Save the trained tokenizer\n",
    "tokenizer.save(\"char_bpe_tokenizer.json\")\n",
    "\n",
    "# Example usage\n",
    "encoded = tokenizer.encode(\"abcd capital .\")\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tokenizes batches of examples into tensors, for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "decode = {v: k for k, v in vocab.items()}\n",
    "\n",
    "max_len = max([len(tokenizer.encode(s).ids) for s in generate_string_manipulation_data(10000)])\n",
    "\n",
    "def data_process(data):\n",
    "    global max_len\n",
    "    result = []\n",
    "    for item in data:\n",
    "        encoded = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "        ids = tokenizer.encode(item).ids\n",
    "        encoded[:len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "        result.append(encoded)\n",
    "    return torch.stack(result)\n",
    "\n",
    "def make_epoch_data(batch_size, epoch_len):\n",
    "    flat_train_data = data_process(\n",
    "        generate_string_manipulation_data(batch_size * epoch_len)\n",
    "    )\n",
    "    return flat_train_data.view(epoch_len, batch_size, -1)\n",
    "\n",
    "make_epoch_data(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "embed_dim = 32\n",
    "num_layers = 5\n",
    "num_epochs = 100\n",
    "warmup_learning_rate = 0.0002\n",
    "learning_rate = 0.002\n",
    "num_batches = 50\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(len(vocab), embed_dim, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=warmup_learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_data = make_epoch_data(batch_size, num_batches).to(device)\n",
    "    for i in range(0, len(epoch_data)):\n",
    "        inputs = epoch_data[i,:,:-1]\n",
    "        targets = epoch_data[i,:,1:]\n",
    "        #print('inputs:', tokenizer.decode(inputs[0].tolist()))\n",
    "        #print('targets', tokenizer.decode(targets[0].tolist()))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), targets.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    if epoch > 3:\n",
    "        optimizer.param_groups[0]['lr'] = learning_rate * (num_epochs - epoch) / num_epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_text = tokenizer.encode(\".\").ids[0]\n",
    "\n",
    "# Inference\n",
    "def generate_text(prompt, max_len=28):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).ids\n",
    "    prompt_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(prompt_tensor)\n",
    "            pred_token = output.argmax(dim=2)[:,-1]\n",
    "            if pred_token == 0:\n",
    "                break\n",
    "            prompt_tensor = torch.cat((prompt_tensor, pred_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "    model.train()\n",
    "    predicted_sentence = ''.join([decode[t.item()] for t in prompt_tensor.squeeze()])\n",
    "    return predicted_sentence\n",
    "\n",
    "# Example usage\n",
    "prompt = \"amazing echo\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
