{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`pip install torch torchtext`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eo1EwQhfQJ0S"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share/u/caden/.conda/envs/work/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/share/u/caden/.conda/envs/work/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/share/u/caden/.conda/envs/work/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import random\n",
        "import math\n",
        "import string\n",
        "\n",
        "from typing import Float, List, Tuple\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In 2017, Google Brain release a paper titled Attention is All you Need, where they departed from the traditional Seq2Seq architectures used in NLP and introduced a new kind of architecture: the transformer. As the title of their paper suggests, the transformer is based on attention mechanisms, and does not use any kind of recurrent states as in RNNs. This meant that a transformer could be trained significantly faster than a Seq2Seq model using RNNs, as sequences were no longer processed one token at a time, but rather all at once. It also significantly reduced the complexity of the models being worked with. As opposed to having complex Seq2Seq variations that can quickly become intractable, the transformer is composed of relatively straightforward matrix multiplications all the way through.\n",
        "\n",
        "Since their introduction, transformers have been taking over the world of NLP. GPT-3, a state-of-the-art language model capable of outputting text nearly impossible to distinguish from something human-written, is a massive transformer model. Transformers have also seen applications in other fields of machine learning like computer vision, as their expresive power is, well, powerful.\n",
        "\n",
        "In this notebook, we'll use our intuition from the previous section to build a transformer from scratch. The notebook will break down each component of the transformer for you to implement and understand yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Transformer Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A transformer consists of a series of `n_layers` blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's look at the attention layer in detail.\n",
        "\n",
        "The job of the attention layer is to allow a token to \"see\" information from faraway long-ago tokens. It does this by forming a \"query\" vector that it uses to match other token \"key\" vectors. Then the information that it sees is put into a set of \"value\" vectors that are summed up and added to the original querying token state.\n",
        "\n",
        "We'll load in our set of saved attention heads from gpt-2 small and take a look at some common patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-d85e0497-4a7a\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-d85e0497-4a7a\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"Bob\", \" and\", \" Alice\", \" went\", \" to\", \" the\", \" store\", \".\", \" Bob\", \" handed\", \" the\", \" milk\", \" to\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9003483653068542, 0.09965165704488754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5077958106994629, 0.24631661176681519, 0.24588756263256073, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4170067012310028, 0.13512027263641357, 0.12164026498794556, 0.32623276114463806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41113513708114624, 0.0970381423830986, 0.1736174374818802, 0.2747209668159485, 0.04348832368850708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5116605162620544, 0.06201045215129852, 0.2167089432477951, 0.14232611656188965, 0.02725902386009693, 0.0400349497795105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4272769093513489, 0.14557518064975739, 0.10508330166339874, 0.1235274076461792, 0.07758180797100067, 0.09629414230585098, 0.024661296978592873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35024216771125793, 0.040570084005594254, 0.198772594332695, 0.19826847314834595, 0.021578162908554077, 0.071166031062603, 0.09903772920370102, 0.020364833995699883, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1983787715435028, 0.09587636590003967, 0.45460206270217896, 0.019760381430387497, 0.04298539459705353, 0.02505805715918541, 0.06106359139084816, 0.05800957977771759, 0.04426582157611847, 0.0, 0.0, 0.0, 0.0], [0.23719212412834167, 0.10177523642778397, 0.0635356605052948, 0.2076282501220703, 0.062232691794633865, 0.05940482020378113, 0.02404305897653103, 0.05616835504770279, 0.032704245299100876, 0.1553155481815338, 0.0, 0.0, 0.0], [0.26576149463653564, 0.026183145120739937, 0.11737753450870514, 0.07533552497625351, 0.012009527534246445, 0.018111029639840126, 0.11383207142353058, 0.021467197686433792, 0.11482330411672592, 0.2137501984834671, 0.021348964422941208, 0.0, 0.0], [0.250099241733551, 0.10174068808555603, 0.08555889874696732, 0.0389682874083519, 0.059747159481048584, 0.08007174730300903, 0.029078714549541473, 0.04442940652370453, 0.07606378942728043, 0.03924393281340599, 0.08303574472665787, 0.11196237057447433, 0.0], [0.17659065127372742, 0.03108922392129898, 0.07252846658229828, 0.14333131909370422, 0.015234355814754963, 0.033909186720848083, 0.07195041328668594, 0.022781848907470703, 0.07047298550605774, 0.1983979493379593, 0.04192933440208435, 0.1020573228597641, 0.01972690410912037]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0023827403783798218, 0.9976171851158142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00029916828498244286, 0.00014184934843797237, 0.9995589852333069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011960341362282634, 0.0034069193061441183, 0.0032790875993669033, 0.9921180605888367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00204449356533587, 0.08161914348602295, 0.0017793087754398584, 0.001253554248251021, 0.913303554058075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001673595979809761, 0.07890450954437256, 0.002536248415708542, 0.0002635388809721917, 0.08863552659749985, 0.8279865980148315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016799198929220438, 0.005731148645281792, 0.007605188060551882, 0.0026801861822605133, 0.0017377050826326013, 0.001902919728308916, 0.97866290807724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003386670956388116, 0.01550678163766861, 0.0007283595623448491, 0.0007876262534409761, 0.00837770290672779, 0.0006234660395421088, 8.681543113198131e-05, 0.9705025553703308, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018555890768766403, 0.0010963156819343567, 0.009082971140742302, 0.002480482216924429, 0.0015377382514998317, 0.00041466797119937837, 0.00017863458197098225, 5.777427941211499e-05, 0.9665956497192383, 0.0, 0.0, 0.0, 0.0], [4.950496804667637e-05, 0.0011889688903465867, 0.0006215550238266587, 0.0014667902141809464, 0.0013254308141767979, 8.802896627457812e-05, 0.00016108776617329568, 0.00021016418759245425, 0.00014154150267131627, 0.9947469830513, 0.0, 0.0, 0.0], [0.0008655962883494794, 0.02796148881316185, 0.0008355484460480511, 9.082096948986873e-05, 0.042044930160045624, 0.4725797474384308, 0.00019422886543907225, 0.0013498378684744239, 0.00034885876812040806, 0.00014990390627644956, 0.45357900857925415, 0.0, 0.0], [0.0004973054747097194, 0.001107512041926384, 0.002914908342063427, 0.0008781199576333165, 0.00029934992198832333, 4.91644605062902e-05, 0.0002601940941531211, 2.7362890250515193e-05, 0.0007163183181546628, 0.000147646147524938, 2.7425983716966584e-05, 0.9930746555328369, 0.0], [0.0010327236959710717, 0.023570317775011063, 0.00035806436790153384, 0.00033903145231306553, 0.47410064935684204, 0.0013421939220279455, 5.5069791415007785e-05, 0.004614077974110842, 0.00026840780628845096, 0.0008020245004445314, 0.0010658676037564874, 6.586862582480535e-05, 0.49238571524620056]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48500487208366394, 0.5149951577186584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.744534969329834, 0.18244123458862305, 0.07302379608154297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45616549253463745, 0.3571537435054779, 0.0400913767516613, 0.1465894728899002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28136804699897766, 0.25128036737442017, 0.06656117737293243, 0.09978867322206497, 0.3010016977787018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21870072185993195, 0.2002837359905243, 0.04254140332341194, 0.14176763594150543, 0.3449256718158722, 0.051780834794044495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37145811319351196, 0.13261674344539642, 0.03261379152536392, 0.1357673853635788, 0.11581792682409286, 0.11549849063158035, 0.09622754156589508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23451702296733856, 0.09941870719194412, 0.02522330917418003, 0.3736776113510132, 0.09855931252241135, 0.04495280608534813, 0.028070542961359024, 0.09558069705963135, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37532901763916016, 0.1237587034702301, 0.085838183760643, 0.0369771271944046, 0.061545100063085556, 0.11084344983100891, 0.03503391519188881, 0.08003278821706772, 0.09064163267612457, 0.0, 0.0, 0.0, 0.0], [0.19356167316436768, 0.09697585552930832, 0.05100212246179581, 0.0796467512845993, 0.12295310944318771, 0.12354773283004761, 0.07652045041322708, 0.1321074217557907, 0.04469951242208481, 0.07898543775081635, 0.0, 0.0, 0.0], [0.12705683708190918, 0.11877419054508209, 0.02951367199420929, 0.1048135757446289, 0.22345872223377228, 0.03297611325979233, 0.029651544988155365, 0.18304765224456787, 0.045981746166944504, 0.0710311308503151, 0.033694759011268616, 0.0, 0.0], [0.2675088942050934, 0.10964791476726532, 0.02017233893275261, 0.04064450040459633, 0.10130365192890167, 0.08954411000013351, 0.0448525995016098, 0.0645490363240242, 0.0657346099615097, 0.035361360758543015, 0.08071909844875336, 0.0799618735909462, 0.0], [0.10440709441900253, 0.09684990346431732, 0.03282853215932846, 0.053008828312158585, 0.13698367774486542, 0.026620104908943176, 0.027813194319605827, 0.23970116674900055, 0.034452639520168304, 0.056253623217344284, 0.026498185470700264, 0.016374072059988976, 0.14820894598960876]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525480508804321, 0.44745197892189026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03538107872009277, 0.00010006517550209537, 0.9645189046859741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03149968013167381, 0.003654496744275093, 0.0016503108199685812, 0.9631954431533813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015584082342684269, 0.03514506295323372, 0.006223172880709171, 0.44271090626716614, 0.500336766242981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02836775779724121, 0.03190261870622635, 0.021220937371253967, 0.08752260357141495, 0.19071048498153687, 0.6402755975723267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00402827700600028, 4.330632509663701e-05, 0.0002394524635747075, 0.0007620117394253612, 0.0006895645637996495, 0.0004922799416817725, 0.9937451481819153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019812943413853645, 0.01749766618013382, 0.0070622810162603855, 0.05639893189072609, 0.07162072509527206, 0.17610113322734833, 0.15078605711460114, 0.5007202625274658, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014466672204434872, 1.5285401104847551e-06, 8.484443242195994e-05, 5.563623199122958e-05, 1.6673799109412357e-05, 1.4684448615298606e-05, 0.00045658639282919466, 5.029781459597871e-05, 0.9848530292510986, 0.0, 0.0, 0.0, 0.0], [0.004575993865728378, 1.1270073628111277e-05, 5.2631683502113447e-05, 0.0003212623414583504, 4.1722090827533975e-05, 6.852448859717697e-05, 0.00118782720528543, 0.0003501885512378067, 0.009770208969712257, 0.9836204051971436, 0.0, 0.0, 0.0], [0.008103039115667343, 0.002194326603785157, 0.0010292107472196221, 0.0037837442941963673, 0.006556439679116011, 0.02525443583726883, 0.02066657319664955, 0.04254481568932533, 0.026053577661514282, 0.09953977167606354, 0.764274001121521, 0.0, 0.0], [0.0028965019155293703, 1.3302005754667334e-06, 6.881989975227043e-05, 4.186424121144228e-05, 7.4540639616316184e-06, 6.328975814540172e-06, 0.0012707391288131475, 0.0001055327447829768, 0.00429479219019413, 0.0034329660702496767, 0.00020807093824259937, 0.9876656532287598, 0.0], [0.0018245766405016184, 0.0004638760583475232, 4.2353633034508675e-05, 0.0031666529830545187, 0.002335737459361553, 0.0028441771864891052, 0.0007855111034587026, 0.004696512594819069, 0.00276335165835917, 0.23397107422351837, 0.09533005952835083, 0.013522041961550713, 0.6382541060447693]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9363245964050293, 0.0636754184961319, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.031415004283189774, 0.002302237320691347, 0.9662827849388123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38307997584342957, 0.06992927938699722, 0.21451176702976227, 0.33247894048690796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09434898942708969, 0.040586426854133606, 0.04554226994514465, 0.6693009734153748, 0.15022137761116028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18825329840183258, 0.06846006959676743, 0.09636591374874115, 0.34089627861976624, 0.14290057122707367, 0.16312386095523834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14496974647045135, 0.014249096624553204, 0.04747474566102028, 0.030456405133008957, 0.013175366446375847, 0.022669943049550056, 0.7270045876502991, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07202325761318207, 0.05945225805044174, 0.02754616178572178, 0.08515065163373947, 0.10168962180614471, 0.10554821789264679, 0.24285568296909332, 0.30573421716690063, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03246920183300972, 0.0006531263352371752, 0.011707677505910397, 0.001436964375898242, 0.0009505365742370486, 0.0013884402578696609, 0.004087673034518957, 0.0025882250629365444, 0.944718062877655, 0.0, 0.0, 0.0, 0.0], [0.021541567519307137, 0.0026061031967401505, 0.009742052294313908, 0.008414160460233688, 0.002985527506098151, 0.0027569427620619535, 0.014598120003938675, 0.015395590104162693, 0.013273429125547409, 0.9086864590644836, 0.0, 0.0, 0.0], [0.06130806729197502, 0.016976550221443176, 0.017540710046887398, 0.0709400400519371, 0.028898052871227264, 0.032693251967430115, 0.08802759647369385, 0.03877611085772514, 0.07814226299524307, 0.39071738719940186, 0.17598000168800354, 0.0, 0.0], [0.019282545894384384, 0.0009819816332310438, 0.00800794456154108, 0.002922037150710821, 0.0011241133324801922, 0.0013606396969407797, 0.004794284701347351, 0.0023621225263923407, 0.00860891118645668, 0.006405410822480917, 0.005769602954387665, 0.9383804798126221, 0.0], [0.020583774894475937, 0.006080050487071276, 0.004446953535079956, 0.08526820689439774, 0.017018888145685196, 0.010347596369683743, 0.01559135876595974, 0.016706712543964386, 0.023574892431497574, 0.27767688035964966, 0.06363272666931152, 0.1278860867023468, 0.3311857581138611]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09335817396640778, 0.9066417813301086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0682385116815567, 1.0427002962387633e-05, 0.9317510724067688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.035803116858005524, 0.00020444639085326344, 0.00018361654656473547, 0.9638088941574097, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05292995274066925, 0.16889090836048126, 0.009935812093317509, 0.01856353133916855, 0.7496798038482666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08369892090559006, 0.21659114956855774, 0.00931540410965681, 0.04971448332071304, 0.14927786588668823, 0.49140214920043945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014290455728769302, 5.59846157557331e-05, 4.7547662688884884e-05, 5.093332219985314e-05, 2.1191008272580802e-06, 5.0979992920474615e-06, 0.9855479001998901, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08442119508981705, 0.2104368507862091, 0.023660240694880486, 0.06734660267829895, 0.11163516342639923, 0.11408205330371857, 0.021435584872961044, 0.3669823110103607, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7294803261756897, 2.773756705209962e-06, 0.0001916291075758636, 3.275493872934021e-05, 1.9646148530227947e-07, 1.040427605403238e-06, 2.310456693521701e-06, 2.9710125204474025e-07, 0.27028873562812805, 0.0, 0.0, 0.0, 0.0], [0.0025120375212281942, 2.058241534541594e-06, 9.939100209521712e-07, 0.0003069945960305631, 1.9738577350381092e-07, 6.585747200915648e-07, 4.310613348934567e-06, 1.4841068818327585e-08, 1.2527289072750136e-06, 0.9971714615821838, 0.0, 0.0, 0.0], [0.034534744918346405, 0.13570855557918549, 0.005447680130600929, 0.038695454597473145, 0.1021198108792305, 0.36550843715667725, 0.01985405944287777, 0.024492420256137848, 0.005823574494570494, 0.008203749544918537, 0.2596115171909332, 0.0, 0.0], [0.0012067966163158417, 1.2440627870091703e-06, 1.820370562199969e-05, 1.0212241249973886e-05, 5.873009811807606e-08, 2.0618729479338072e-07, 7.158752396208001e-06, 4.269760367492381e-08, 4.806271363122505e-07, 1.4271469126470038e-06, 8.890699376706834e-08, 0.9987541437149048, 0.0], [0.010359746403992176, 0.06663398444652557, 0.0036313124001026154, 0.010033094324171543, 0.38329365849494934, 0.1460271030664444, 0.002100392011925578, 0.018664808943867683, 0.0012225403916090727, 0.001822581747546792, 0.10383161157369614, 0.0005980872665531933, 0.2517811059951782]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9865140318870544, 0.013485926203429699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.674713134765625, 0.16632676124572754, 0.15896010398864746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6607992053031921, 0.05955777317285538, 0.23649081587791443, 0.04315214976668358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33057188987731934, 0.033936645835638046, 0.1343957930803299, 0.09089521318674088, 0.41020047664642334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5897740125656128, 0.009311742149293423, 0.3137999176979065, 0.0729261264204979, 0.010367650538682938, 0.003820547368377447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5658420324325562, 0.04932893440127373, 0.19286414980888367, 0.032263997942209244, 0.017700105905532837, 0.023427767679095268, 0.11857302486896515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6045771241188049, 0.011848881840705872, 0.19959186017513275, 0.038751546293497086, 0.011336296796798706, 0.0059501780197024345, 0.12405109405517578, 0.0038930093869566917, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3182838559150696, 0.07221677154302597, 0.19999657571315765, 0.02834702469408512, 0.04798002913594246, 0.04968348518013954, 0.10488013178110123, 0.04207539185881615, 0.13653670251369476, 0.0, 0.0, 0.0, 0.0], [0.2908182740211487, 0.05398599058389664, 0.11574599891901016, 0.036660004407167435, 0.02894015982747078, 0.04998013749718666, 0.17781221866607666, 0.03186575695872307, 0.11197587102651596, 0.10221561789512634, 0.0, 0.0, 0.0], [0.3298095762729645, 0.00384817598387599, 0.19684654474258423, 0.041863299906253815, 0.0048384759575128555, 0.0015937343705445528, 0.10868125408887863, 0.001965946052223444, 0.22404716908931732, 0.08471129834651947, 0.0017944744322448969, 0.0, 0.0], [0.19281435012817383, 0.06977643072605133, 0.09640467166900635, 0.04132643714547157, 0.030664077028632164, 0.04677637293934822, 0.08928431570529938, 0.04853367432951927, 0.07376820594072342, 0.07760470360517502, 0.04825630784034729, 0.18479040265083313, 0.0], [0.11978919059038162, 0.009141428396105766, 0.05549226701259613, 0.039156194776296616, 0.16812273859977722, 0.003869706764817238, 0.06332665681838989, 0.005867905914783478, 0.04728712886571884, 0.0930890366435051, 0.004816846456378698, 0.0864577665925026, 0.30358314514160156]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9447391033172607, 0.05526082590222359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5175012946128845, 0.33391833305358887, 0.14858032763004303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19110386073589325, 0.251499742269516, 0.33688220381736755, 0.2205142229795456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17798005044460297, 0.13231772184371948, 0.15075361728668213, 0.29846638441085815, 0.24048225581645966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10391362011432648, 0.09025425463914871, 0.08895783126354218, 0.16510187089443207, 0.23983930051326752, 0.31193313002586365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12432344257831573, 0.05525617673993111, 0.02297692745923996, 0.1913495808839798, 0.19454516470432281, 0.3103664517402649, 0.10118225961923599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06740633398294449, 0.03445212170481682, 0.04983817785978317, 0.06569897383451462, 0.07144302129745483, 0.1463826447725296, 0.17668461799621582, 0.3880941569805145, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05569881945848465, 0.029839128255844116, 0.04218096658587456, 0.03227546438574791, 0.07516519725322723, 0.14144866168498993, 0.15724967420101166, 0.3625674843788147, 0.10357458144426346, 0.0, 0.0, 0.0, 0.0], [0.07005494832992554, 0.02430928498506546, 0.017889080569148064, 0.027805738151073456, 0.06793754547834396, 0.09324196726083755, 0.023594262078404427, 0.3608195185661316, 0.25381746888160706, 0.06053018942475319, 0.0, 0.0, 0.0], [0.03110313042998314, 0.012803011573851109, 0.01336329523473978, 0.021129867061972618, 0.026773735880851746, 0.03668216988444328, 0.05257159844040871, 0.22634464502334595, 0.10255134850740433, 0.14027510583400726, 0.33640214800834656, 0.0, 0.0], [0.017206761986017227, 0.011219529435038567, 0.0020211657974869013, 0.013223971240222454, 0.03573331981897354, 0.04120934382081032, 0.024580929428339005, 0.1581859588623047, 0.03460468351840973, 0.2852962911128998, 0.3132070302963257, 0.063510961830616, 0.0], [0.026869768276810646, 0.007560096215456724, 0.00824764184653759, 0.014783160760998726, 0.009929080493748188, 0.021598221734166145, 0.02515270933508873, 0.09505569934844971, 0.0810939222574234, 0.06973497569561005, 0.19004133343696594, 0.12421346455812454, 0.325719952583313]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3248785138130188, 0.6751214861869812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.622121274471283, 0.27012911438941956, 0.10774964094161987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34321364760398865, 0.48904451727867126, 0.09039457887411118, 0.0773472785949707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.033615630120038986, 0.3402939736843109, 0.010385239496827126, 0.07333160936832428, 0.5423735976219177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021822532638907433, 0.16573511064052582, 0.008527790196239948, 0.054838672280311584, 0.22749586403369904, 0.5215800404548645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24727900326251984, 0.1943882256746292, 0.07926817983388901, 0.12468091398477554, 0.14437463879585266, 0.1501515805721283, 0.059857506304979324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.023024972528219223, 0.1516420692205429, 0.0062642404809594154, 0.014259357005357742, 0.13136844336986542, 0.18354953825473785, 0.009212588891386986, 0.48067882657051086, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19693665206432343, 0.14076925814151764, 0.12241804599761963, 0.04412658512592316, 0.10309427231550217, 0.16258738934993744, 0.04915281757712364, 0.1105252206325531, 0.0703897625207901, 0.0, 0.0, 0.0, 0.0], [0.0753081813454628, 0.14089782536029816, 0.10668250918388367, 0.05950472131371498, 0.12903937697410583, 0.20867078006267548, 0.050099629908800125, 0.12418827414512634, 0.021749479696154594, 0.08385921269655228, 0.0, 0.0, 0.0], [0.00893303845077753, 0.07054587453603745, 0.00392918149009347, 0.027648013085126877, 0.09800022095441818, 0.22597840428352356, 0.010474647395312786, 0.17806470394134521, 0.011955383233726025, 0.016983089968562126, 0.3474874496459961, 0.0, 0.0], [0.13305488228797913, 0.1119953915476799, 0.06944822520017624, 0.04010802134871483, 0.10983814299106598, 0.07718504965305328, 0.12846866250038147, 0.06319262087345123, 0.028898023068904877, 0.028296098113059998, 0.07493630796670914, 0.13457860052585602, 0.0], [0.0058413781225681305, 0.06911439448595047, 0.002289031632244587, 0.019347520545125008, 0.11652383208274841, 0.15757513046264648, 0.007416361011564732, 0.12680105865001678, 0.004333107732236385, 0.012139043770730495, 0.2342255413532257, 0.024790722876787186, 0.21960289776325226]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6182527542114258, 0.38174721598625183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5875615477561951, 0.36056283116340637, 0.051875628530979156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3816367983818054, 0.3155573010444641, 0.14224040508270264, 0.16056549549102783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2800033390522003, 0.2718298137187958, 0.08587689697742462, 0.11728968471288681, 0.2450002282857895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24108004570007324, 0.21517245471477509, 0.0890621468424797, 0.10090535879135132, 0.19423578679561615, 0.1595441699028015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2622065544128418, 0.20335723459720612, 0.10229724645614624, 0.0929705798625946, 0.1551474928855896, 0.15580891072750092, 0.028211934491991997, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18770679831504822, 0.1697266399860382, 0.08337301760911942, 0.10222207009792328, 0.1435418576002121, 0.12200433015823364, 0.058684416115283966, 0.1327408403158188, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13415536284446716, 0.18032126128673553, 0.11307669430971146, 0.06382941454648972, 0.12875637412071228, 0.14815840125083923, 0.08182210475206375, 0.12610559165477753, 0.02377478778362274, 0.0, 0.0, 0.0, 0.0], [0.15511125326156616, 0.1308683454990387, 0.07985959947109222, 0.09187661111354828, 0.11570025980472565, 0.09934698790311813, 0.08875322341918945, 0.12098435312509537, 0.07961482554674149, 0.03788456320762634, 0.0, 0.0, 0.0], [0.14530807733535767, 0.13308589160442352, 0.06300481408834457, 0.07067154347896576, 0.122052863240242, 0.10121855139732361, 0.0448354110121727, 0.10093418508768082, 0.058333780616521835, 0.05249560996890068, 0.10805924981832504, 0.0, 0.0], [0.13511763513088226, 0.12634159624576569, 0.06744084507226944, 0.0628521665930748, 0.0978514552116394, 0.0921369194984436, 0.04510851576924324, 0.1007256880402565, 0.05053048953413963, 0.1169915497303009, 0.0939607247710228, 0.010942373424768448, 0.0], [0.10865935683250427, 0.11043550819158554, 0.04260362684726715, 0.05842597410082817, 0.10303770005702972, 0.09051069617271423, 0.041422177106142044, 0.08769222348928452, 0.04675551876425743, 0.052661411464214325, 0.09797852486371994, 0.04177491366863251, 0.11804244667291641]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6094799637794495, 0.3905200660228729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48680561780929565, 0.21682481467723846, 0.29636961221694946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3415258526802063, 0.29469940066337585, 0.0659882202744484, 0.29778653383255005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2530001699924469, 0.33498379588127136, 0.08016787469387054, 0.10572809725999832, 0.22612011432647705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2623036503791809, 0.2386792153120041, 0.09439867734909058, 0.0555427223443985, 0.13807840645313263, 0.21099737286567688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14997930824756622, 0.18675822019577026, 0.06411851197481155, 0.07587393373250961, 0.1268763542175293, 0.11408069729804993, 0.2823130190372467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1725109964609146, 0.21122585237026215, 0.05828040465712547, 0.10090997815132141, 0.15163615345954895, 0.10200659185647964, 0.0367511510848999, 0.166678786277771, 0.0, 0.0, 0.0, 0.0, 0.0], [0.27796670794487, 0.09915798157453537, 0.12724705040454865, 0.054820913821458817, 0.07898680865764618, 0.07994212210178375, 0.024836989119648933, 0.05709182471036911, 0.1999496966600418, 0.0, 0.0, 0.0, 0.0], [0.10487959533929825, 0.11986104398965836, 0.04590247943997383, 0.11273248493671417, 0.09793101996183395, 0.07962905615568161, 0.040072884410619736, 0.08040537685155869, 0.03123198263347149, 0.28735408186912537, 0.0, 0.0, 0.0], [0.1474006623029709, 0.12895911931991577, 0.07326046377420425, 0.045425817370414734, 0.1025700494647026, 0.17154856026172638, 0.03858625143766403, 0.07038328796625137, 0.04226132854819298, 0.03092610463500023, 0.14867834746837616, 0.0, 0.0], [0.1417132169008255, 0.11490071564912796, 0.06451448053121567, 0.048572804778814316, 0.10178765654563904, 0.07461176067590714, 0.031678229570388794, 0.07109073549509048, 0.03749796748161316, 0.02728649042546749, 0.057013530284166336, 0.22933243215084076, 0.0], [0.0903700441122055, 0.1175965890288353, 0.047939591109752655, 0.07466825097799301, 0.13414081931114197, 0.09339521080255508, 0.03269520029425621, 0.08621606230735779, 0.038981154561042786, 0.04574466124176979, 0.0844951793551445, 0.03469683974981308, 0.11906035989522934]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48765626549720764, 0.51234370470047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3438650667667389, 0.5568835139274597, 0.09925145655870438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4328292906284332, 0.3359185755252838, 0.08071672916412354, 0.15053540468215942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33066466450691223, 0.2129710465669632, 0.0876198410987854, 0.1441197693347931, 0.22462470829486847, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2972277104854584, 0.15985049307346344, 0.06966746598482132, 0.12172845751047134, 0.16916239261627197, 0.18236351013183594, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32177597284317017, 0.16009995341300964, 0.057227909564971924, 0.1140822246670723, 0.14520609378814697, 0.059422239661216736, 0.14218559861183167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2414333075284958, 0.1247822716832161, 0.06727145612239838, 0.09830164909362793, 0.11759821325540543, 0.06543677300214767, 0.09218187630176544, 0.19299443066120148, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16298925876617432, 0.10644740611314774, 0.05408326908946037, 0.12383709847927094, 0.07709842920303345, 0.06768395751714706, 0.16305921971797943, 0.19103753566741943, 0.05376385524868965, 0.0, 0.0, 0.0, 0.0], [0.20554642379283905, 0.15155331790447235, 0.039707258343696594, 0.07266402989625931, 0.16453832387924194, 0.06271219253540039, 0.03800979256629944, 0.13991887867450714, 0.04555608704686165, 0.0797937661409378, 0.0, 0.0, 0.0], [0.17125946283340454, 0.07754144817590714, 0.043873805552721024, 0.06861068308353424, 0.08239035308361053, 0.09009739011526108, 0.06527110934257507, 0.12786339223384857, 0.055237822234630585, 0.11940048635005951, 0.09845396876335144, 0.0, 0.0], [0.13761313259601593, 0.08618729561567307, 0.06556284427642822, 0.0991603434085846, 0.08287505805492401, 0.04933508485555649, 0.07261829823255539, 0.12134094536304474, 0.07552524656057358, 0.11049164831638336, 0.05609184876084328, 0.043198224157094955, 0.0], [0.17508669197559357, 0.08580853044986725, 0.05349905416369438, 0.07348812371492386, 0.09152444452047348, 0.045669618993997574, 0.057753171771764755, 0.10385310649871826, 0.0501100979745388, 0.0820503905415535, 0.04582506790757179, 0.04005046188831329, 0.095281220972538]]]}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7febcf556260>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import circuitsvis as cv\n",
        "\n",
        "str_tokens = torch.load('tokens.pt')\n",
        "attn_patterns = torch.load('attn_patterns.pt')\n",
        "\n",
        "display(\n",
        "    cv.attention.attention_patterns(\n",
        "        tokens=str_tokens, \n",
        "        attention=attn_patterns.squeeze(0)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a look at some of the patterns.\n",
        "\n",
        "We notice that there are three basic patterns which repeat quite frequently:\n",
        "\n",
        "* `prev_token_heads`, which attend mainly to the previous token (e.g. head `0.7`)\n",
        "* `current_token_heads`, which attend mainly to the current token (e.g. head `0.1`)\n",
        "* `first_token_heads`, which attend mainly to the first token (e.g. heads `0.0`, although these are a bit less clear-cut than the other two)\n",
        "\n",
        "The `prev_token_heads` and `current_token_heads` are perhaps unsurprising, because words that are close together in a sequence probably have a lot more mutual information (i.e. we could get quite far using bigram or trigram prediction).\n",
        "\n",
        "The `first_token_heads` are a bit more surprising. The basic intuition here is that the first token in a sequence is often used as a resting or null position for heads that only sometimes activate (since our attention probabilities always have to add up to 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{equation}\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgqYZmyO-muh"
      },
      "outputs": [],
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        \"\"\"\n",
        "        Here we will assume that the input dimensions are same as the\n",
        "        output dims.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # SOLUTION\n",
        "        self.q_layer = torch.nn.Linear(d, d)\n",
        "        self.k_layer = torch.nn.Linear(d, d)\n",
        "        self.v_layer = torch.nn.Linear(d, d)\n",
        "\n",
        "    def forward(self, x, mask=None, return_weights=False):\n",
        "        \"\"\"\n",
        "        Assume x is <t x d> -- t being the sequence length, d\n",
        "        the embed dims.\n",
        "\n",
        "        W_q, W_k, and W_v are weights for projecting into queries,\n",
        "        keys, and values, respectively. Here these will have shape\n",
        "        <d x t>, yielding d dimensional vectors for each input.\n",
        "\n",
        "        This function should return a t dimensional attention vector\n",
        "        for each input -- i.e., an attention matrix with shape <t x t>,\n",
        "        and the values derived from this <t x d>.\n",
        "\n",
        "        Derive Q, K, V matrices, then self attention weights. These should\n",
        "        be used to compute the final representations (t x d); optionally\n",
        "        return the weights matrix if `return_weights=True`.\n",
        "        \"\"\"\n",
        "        # SOLUTION\n",
        "        Q = self.q_layer(x)\n",
        "        K = self.k_layer(x)\n",
        "        V = self.v_layer(x)\n",
        "\n",
        "        A = Q @ K.transpose(-2, -1)\n",
        "        if mask is not None:\n",
        "            A = A.masked_fill(mask == 0, -1e9)\n",
        "        weights = F.softmax(A, dim=-1)\n",
        "\n",
        "        if return_weights:\n",
        "          return weights, weights @ V\n",
        "\n",
        "        return weights @ V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can all be done in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{equation}\n",
        "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.w = nn.Parameter(torch.ones(d_model))\n",
        "        self.b = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
        "        # SOLUTION\n",
        "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
        "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.eps).sqrt()\n",
        "\n",
        "        residual = (residual - residual_mean) / residual_std\n",
        "        return residual * self.w + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.self_attn = SingleHeadAttention(embed_dim)\n",
        "        self.norm1 = LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        src = src + self.norm1(self.self_attn(src, mask=mask))\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(100.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuRuwq1Q_Z4x"
      },
      "source": [
        "Now we put lots of attention layers into a transformer model.\n",
        "\n",
        "Around each attention layer we create a residual structure, and we also use LayerNorm to prevent things from blowing up.\n",
        "\n",
        "The last detail is Positional encoding - a vector we add to each token based on where it is, rather than what it is.  This allows a token to be \"queried\" according to position rather than content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEbCUxGivkmA"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
        "        self.layers = nn.ModuleList([SelfAttentionLayer(embed_dim) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.decoder = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        mask = torch.triu(torch.ones(\n",
        "            src.size(1), src.size(1), dtype=torch.bool, device=src.device)).T\n",
        "        src = self.embed(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        src = self.norm(src)\n",
        "        output = self.decoder(src)\n",
        "        return output\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
